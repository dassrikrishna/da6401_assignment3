# DA6401 Assignment-3
#### Detailed Weights & Biases Report for My Project: [Click Here](https://api.wandb.ai/links/ma24m025-indian-institute-of-technology-madras/9y9edofe)
#### Github Link: [Click Here](https://github.com/dassrikrishna/da6401_assignment3)
#### Seq2Seq Vanila: [Click Here](https://github.com/dassrikrishna/da6401_assignment3/blob/main/da6401-assignment3-vanila.ipynb)
#### Seq2Seq Attention: [Click Here](https://github.com/dassrikrishna/da6401_assignment3/blob/main/da6401-assignment3-attention.ipynb)
#### predictions_vanilla: [Click Here](https://github.com/dassrikrishna/da6401_assignment3/tree/main/predictions_vanilla)
#### predictions_attention [Click Here](https://github.com/dassrikrishna/da6401_assignment3/tree/main/predictions_attention)

## DEEP LEARNING
#### ```SRIKRISHNA DAS (MA24M025)```
#### `M.Tech (Industrial Mathematics and Scientific Computing) IIT Madras`
 

## [Problem Statement](https://wandb.ai/sivasankar1234/DA6401/reports/Assignment-3--VmlldzoxMjM4MjYzMg)

## Goal of the Assignment
The primary objectives of this assignment are as follows:

### 1.Model Sequence-to-Sequence Learning Problems
Understand how to model sequence-to-sequence (seq2seq) learning tasks using Recurrent Neural Networks (RNNs).

### 2.Compare Different RNN Cells
Explore and compare the performance of different RNN cell architectures including:
- Vanilla RNN
- Long Short-Term Memory (LSTM)
- Gated Recurrent Unit (GRU)
  
### 3.Understand Attention Mechanisms
Learn how attention mechanisms help overcome the limitations of traditional seq2seq models by improving context representation and performance.

### 4.Visualize RNN Components
Gain insights into the internal workings of RNN-based models by visualizing the interactions between various components during the learning process.
