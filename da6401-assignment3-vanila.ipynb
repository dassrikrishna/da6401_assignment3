{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:20.685063Z",
     "iopub.status.busy": "2025-05-19T17:46:20.684454Z",
     "iopub.status.idle": "2025-05-19T17:46:24.296353Z",
     "shell.execute_reply": "2025-05-19T17:46:24.291257Z",
     "shell.execute_reply.started": "2025-05-19T17:46:20.685031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\srikr\\anaconda3\\lib\\site-packages (0.19.8)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\srikr\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Initial Setup\n",
    "\n",
    "Importing the necessary libraries and setting a global random seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.300644Z",
     "iopub.status.busy": "2025-05-19T17:46:24.300020Z",
     "iopub.status.idle": "2025-05-19T17:46:24.317114Z",
     "shell.execute_reply": "2025-05-19T17:46:24.311229Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.300613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import IPython.display as display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.font_manager as fm\n",
    "import warnings\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "\n",
    "Load the dataset, build vocabularies, and define helper functions for sequence conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.318743Z",
     "iopub.status.busy": "2025-05-19T17:46:24.318530Z",
     "iopub.status.idle": "2025-05-19T17:46:24.554819Z",
     "shell.execute_reply": "2025-05-19T17:46:24.549046Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.318722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 94546, Val: 9279, Test: 9228\n",
      "Input vocab: 29, Target vocab: 63\n",
      "Max input length: 22, Max target length: 24\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                bengali, latin = parts[0], parts[1]\n",
    "                data.append((latin, bengali))\n",
    "    return data\n",
    "    \n",
    "# Special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "# File paths\n",
    "train_path = '/kaggle/input/dakshina-dataset-v1-0/bn/lexicons/bn.translit.sampled.train.tsv'\n",
    "val_path = '/kaggle/input/dakshina-dataset-v1-0/bn/lexicons/bn.translit.sampled.dev.tsv'\n",
    "test_path = '/kaggle/input/dakshina-dataset-v1-0/bn/lexicons/bn.translit.sampled.test.tsv'\n",
    "\n",
    "train_data = load_data(train_path)\n",
    "val_data = load_data(val_path)\n",
    "test_data = load_data(test_path)\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(sequences):\n",
    "    chars = sorted(set(\"\".join(sequences)))\n",
    "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + chars\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "# Extract input and target sequences\n",
    "input_texts = [x[0] for x in train_data]\n",
    "target_texts = [x[1] for x in train_data]\n",
    "\n",
    "input_char2idx, input_idx2char = build_vocab(input_texts)\n",
    "target_char2idx, target_idx2char = build_vocab(target_texts)\n",
    "\n",
    "INPUT_DIM = len(input_char2idx)\n",
    "OUTPUT_DIM = len(target_char2idx)\n",
    "PAD_IDX = target_char2idx[PAD_TOKEN]\n",
    "SOS_IDX = target_char2idx[SOS_TOKEN]\n",
    "EOS_IDX = target_char2idx[EOS_TOKEN]\n",
    "\n",
    "max_input_len = max(len(seq) for seq in input_texts)\n",
    "max_target_len = max(len(seq) for seq in target_texts) + 2  # for <sos> and <eos>\n",
    "\n",
    "print(f\"Input vocab: {INPUT_DIM}, Target vocab: {OUTPUT_DIM}\")\n",
    "print(f\"Max input length: {max_input_len}, Max target length: {max_target_len}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset and DataLoader\n",
    "\n",
    "Define a PyTorch `Dataset` class to handle the sequence-to-sequence data, and instantiate `DataLoader`s for train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.557269Z",
     "iopub.status.busy": "2025-05-19T17:46:24.557029Z",
     "iopub.status.idle": "2025-05-19T17:46:24.574834Z",
     "shell.execute_reply": "2025-05-19T17:46:24.570128Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.557247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, data, input_char2idx, target_char2idx, max_input_len, max_target_len):\n",
    "        self.data = data\n",
    "        self.input_char2idx = input_char2idx\n",
    "        self.target_char2idx = target_char2idx\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def encode_input(self, seq):\n",
    "        idxs = [self.input_char2idx.get(char, self.input_char2idx[PAD_TOKEN]) for char in seq]\n",
    "        padding = [self.input_char2idx[PAD_TOKEN]] * (self.max_input_len - len(idxs))\n",
    "        return torch.tensor(idxs + padding, dtype=torch.long)\n",
    "\n",
    "    def encode_target(self, seq):\n",
    "        full_seq = [SOS_TOKEN] + list(seq) + [EOS_TOKEN]\n",
    "        idxs = [self.target_char2idx.get(char, self.target_char2idx[PAD_TOKEN]) for char in full_seq]\n",
    "        padding = [self.target_char2idx[PAD_TOKEN]] * (self.max_target_len - len(idxs))\n",
    "        return torch.tensor(idxs + padding, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latin, bengali = self.data[idx]\n",
    "        return self.encode_input(latin), self.encode_target(bengali)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "train_dataset = TransliterationDataset(train_data, input_char2idx, target_char2idx, max_input_len, max_target_len)\n",
    "val_dataset = TransliterationDataset(val_data, input_char2idx, target_char2idx, max_input_len, max_target_len)\n",
    "test_dataset = TransliterationDataset(test_data, input_char2idx, target_char2idx, max_input_len, max_target_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Architecture\n",
    "\n",
    "Define the Encoder, Decoder, and Seq2Seq (sequence-to-sequence) models, supporting multiple cell types (RNN, GRU, LSTM) and beam search decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.577307Z",
     "iopub.status.busy": "2025-05-19T17:46:24.577082Z",
     "iopub.status.idle": "2025-05-19T17:46:24.605706Z",
     "shell.execute_reply": "2025-05-19T17:46:24.599891Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.577286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, layers=1, dropout=0.5, cell='gru'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        rnn = {'lstm': nn.LSTM, 'gru': nn.GRU, 'rnn': nn.RNN}[cell]\n",
    "        self.rnn = rnn(embed_size, hidden_size, layers, dropout=dropout if layers > 1 else 0, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, layers=1, dropout=0.5, cell='gru'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        rnn = {'lstm': nn.LSTM, 'gru': nn.GRU, 'rnn': nn.RNN}[cell]\n",
    "        self.rnn = rnn(embed_size, hidden_size, layers, dropout=dropout if layers > 1 else 0, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return self.fc(output.squeeze(1)), hidden\n",
    "\n",
    "    def beam_search(self, hidden, beam_width, sos_idx, eos_idx, max_len, device):\n",
    "        sequences = [([sos_idx], hidden, 0.0)]\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, h, score in sequences:\n",
    "                if seq[-1] == eos_idx:\n",
    "                    all_candidates.append((seq, h, score))\n",
    "                    continue\n",
    "                input_token = torch.tensor([seq[-1]], device=device)\n",
    "                output, h_new = self.forward(input_token, h)\n",
    "                log_probs = F.log_softmax(output, dim=1).squeeze(0)\n",
    "                top_log_probs, top_tokens = torch.topk(log_probs, beam_width)\n",
    "                for i in range(beam_width):\n",
    "                    candidate = seq + [top_tokens[i].item()]\n",
    "                    candidate_score = score + top_log_probs[i].item()\n",
    "                    all_candidates.append((candidate, h_new, candidate_score))\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[2], reverse=True)[:beam_width]\n",
    "            if all(seq[-1] == eos_idx for seq, _, _ in sequences):\n",
    "                break\n",
    "        return sequences[0][0], sequences[0][2]\n",
    "\n",
    "# Adjust hidden states if encoder/decoder layers mismatch\n",
    "def adjust_hidden_state(enc_hidden, enc_layers, dec_layers, hidden_size, cell):\n",
    "    def pad_tensor(tensor):\n",
    "        if dec_layers == enc_layers:\n",
    "            return tensor\n",
    "        batch_size = tensor.size(1)\n",
    "        device = tensor.device\n",
    "        extra = torch.zeros(abs(dec_layers - enc_layers), batch_size, hidden_size, device=device)\n",
    "        return torch.cat([tensor, extra], dim=0) if dec_layers > enc_layers else tensor[-dec_layers:]\n",
    "    if cell == 'lstm':\n",
    "        return (pad_tensor(enc_hidden[0]), pad_tensor(enc_hidden[1]))\n",
    "    return pad_tensor(enc_hidden)\n",
    "\n",
    "# Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size, hidden_size, enc_layers=1, dec_layers=1, dropout=0.3, cell='gru'):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, embed_size, hidden_size, enc_layers, dropout, cell)\n",
    "        self.decoder = Decoder(output_size, embed_size, hidden_size, dec_layers, dropout, cell)\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = cell\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size, trg_len = trg.size()\n",
    "        outputs = torch.zeros(batch_size, trg_len, self.output_size).to(src.device)\n",
    "        enc_hidden = self.encoder(src)\n",
    "        dec_hidden = adjust_hidden_state(enc_hidden, self.enc_layers, self.dec_layers, self.hidden_size, self.cell)\n",
    "        dec_input = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            dec_input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "        return outputs\n",
    "\n",
    "    def inference_beam_search(self, src, beam_width, sos_idx, eos_idx, max_len, device):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            enc_hidden = self.encoder(src)\n",
    "            dec_hidden = adjust_hidden_state(enc_hidden, self.enc_layers, self.dec_layers, self.hidden_size, self.cell)\n",
    "            return self.decoder.beam_search(dec_hidden, beam_width, sos_idx, eos_idx, max_len, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Evaluation Functions\n",
    "\n",
    "Standard training, validation, and test routines for sequence-to-sequence models. Metrics and loss are computed and returned for each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.608034Z",
     "iopub.status.busy": "2025-05-19T17:46:24.607820Z",
     "iopub.status.idle": "2025-05-19T17:46:24.623365Z",
     "shell.execute_reply": "2025-05-19T17:46:24.618879Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.608013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, trg in dataloader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            total_loss += criterion(output, trg).item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def sequence_accuracy(model, dataloader, target_idx2char, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            for i in range(src.size(0)):\n",
    "                input_seq = src[i].unsqueeze(0)\n",
    "                pred_seq, _ = model.inference_beam_search(input_seq, beam_width=3, sos_idx=SOS_IDX, eos_idx=EOS_IDX, max_len=trg.size(1), device=device)\n",
    "                pred_str = ''.join([target_idx2char[idx] for idx in pred_seq if idx not in (PAD_IDX, SOS_IDX, EOS_IDX)])\n",
    "                true_str = ''.join([target_idx2char[idx] for idx in trg[i].tolist() if idx not in (PAD_IDX, SOS_IDX, EOS_IDX)])\n",
    "                correct += (pred_str == true_str)\n",
    "                total += 1\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main Training Loop with Weights & Biases (W&B)\n",
    "\n",
    "The `main` function can be used both for single runs and as part of W&B hyperparameter sweeps.  \n",
    "It logs results to W&B and saves the best model by validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.625612Z",
     "iopub.status.busy": "2025-05-19T17:46:24.625370Z",
     "iopub.status.idle": "2025-05-19T17:46:24.637794Z",
     "shell.execute_reply": "2025-05-19T17:46:24.633231Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.625591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize W&B\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Set run name dynamically\n",
    "    run_name = f\"emb{config.emb_dim}_hid{config.hidden_dim}_enc{config.encoder_layers}_dec{config.decoder_layers}_drop{config.dropout}_{config.cell}_beam{config.beam_size}\"\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    # Create model with sweep-configurable parameters\n",
    "    model = Seq2Seq(\n",
    "        input_size=INPUT_DIM,\n",
    "        output_size=OUTPUT_DIM,\n",
    "        embed_size=config.emb_dim,\n",
    "        hidden_size=config.hidden_dim,\n",
    "        enc_layers=config.encoder_layers,\n",
    "        dec_layers=config.decoder_layers,\n",
    "        dropout=config.dropout,\n",
    "        cell=config.cell.lower()\n",
    "    ).to(device)\n",
    "\n",
    "    # Set up optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    for epoch in range(10):  # Max 10 epochs\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_acc = sequence_accuracy(model, train_loader, target_idx2char, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        val_acc = sequence_accuracy(model, val_loader, target_idx2char, device)\n",
    "\n",
    "        # Logging\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\":train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "        print(f\"Epoch [{epoch+1}/10],Train Accuracy: {train_acc:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save each model with run id\n",
    "    model_path = f\"model_{wandb.run.id}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    wandb.save(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Log in to W&B Account\n",
    "\n",
    "Login into my W&B Account with API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:24.639997Z",
     "iopub.status.busy": "2025-05-19T17:46:24.639790Z",
     "iopub.status.idle": "2025-05-19T17:46:25.979841Z",
     "shell.execute_reply": "2025-05-19T17:46:25.973702Z",
     "shell.execute_reply.started": "2025-05-19T17:46:24.639979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma24m025\u001b[0m (\u001b[33mma24m025-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='150002a34bcf7d04848ccaff65ab76ca5cc3f11b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Weights & Biases Sweep Configuration\n",
    "\n",
    "Set up a Bayesian parameter sweep for hyperparameter tuning with early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:22:56.031210Z",
     "iopub.status.busy": "2025-05-18T04:22:56.030603Z",
     "iopub.status.idle": "2025-05-18T05:54:49.438404Z",
     "shell.execute_reply": "2025-05-18T05:54:49.437534Z",
     "shell.execute_reply.started": "2025-05-18T04:22:56.031187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: v6vdkkyt\n",
      "Sweep URL: https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/sweeps/v6vdkkyt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ad8xh213 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_042303-ad8xh213</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/runs/ad8xh213' target=\"_blank\">autumn-sweep-1</a></strong> to <a href='https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/sweeps/v6vdkkyt' target=\"_blank\">https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/sweeps/v6vdkkyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq' target=\"_blank\">https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/sweeps/v6vdkkyt' target=\"_blank\">https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/sweeps/v6vdkkyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/runs/ad8xh213' target=\"_blank\">https://wandb.ai/ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/runs/ad8xh213</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Accuracy: 0.0013 | Val Accuracy: 0.0013\n",
      "Epoch [2/10],Train Accuracy: 0.0938 | Val Accuracy: 0.0822\n",
      "Epoch [3/10],Train Accuracy: 0.2358 | Val Accuracy: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# Sweep Config\n",
    "sweep_config = {\n",
    "    'method': \"bayes\",\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'early_terminate': {'type': \"hyperband\", 'min_iter': 3},\n",
    "    'parameters': {\n",
    "        'emb_dim': {'values': [16, 32, 64, 128, 256]},\n",
    "        'hidden_dim': {'values': [16, 32, 64, 128, 256]},\n",
    "        'encoder_layers': {'values': [1, 2, 3]},\n",
    "        'decoder_layers': {'values': [1, 2, 3]},\n",
    "        'dropout': {'values': [0.2, 0.3]},\n",
    "        'cell': {'values': ['RNN', 'GRU', 'LSTM']},\n",
    "        'beam_size': {'values': [1, 3, 5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dakshina-seq2seq\")\n",
    "wandb.agent(sweep_id, function=main, count=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Load Best Model and Evaluate on Test Set & Save Predictions\n",
    "\n",
    "### Load the best model from sweep, evaluate on test set,\n",
    "### generate sample predictions, and save them to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:35.713029Z",
     "iopub.status.busy": "2025-05-19T17:46:35.712514Z",
     "iopub.status.idle": "2025-05-19T17:46:36.164255Z",
     "shell.execute_reply": "2025-05-19T17:46:36.159833Z",
     "shell.execute_reply.started": "2025-05-19T17:46:35.713002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.val_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Run ID: vb1yhddm\n",
      "Best Validation Accuracy: 0.3448\n",
      "Best Hyperparameters:\n",
      "{'cell': 'LSTM', 'dropout': 0.2, 'emb_dim': 64, 'beam_size': 3, 'hidden_dim': 256, 'decoder_layers': 2, 'encoder_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "# Initialize API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Sweep path\n",
    "sweep_path = \"ma24m025-indian-institute-of-technology-madras/dakshina-seq2seq/sweeps/57m8zy04\"\n",
    "# sweep_path = \"your_username/dakshina-seq2seq/sweeps/your_sweep_id\" \n",
    "\n",
    "# Get best run & Corrosponding config\n",
    "sweep = api.sweep(sweep_path)\n",
    "best_run = sweep.best_run(order='val_accuracy')\n",
    "best_config = best_run.config\n",
    "\n",
    "# Print all details\n",
    "print(f\"Best Run ID: {best_run.id}\")\n",
    "print(f\"Best Validation Accuracy: {best_run.summary['val_accuracy']:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:46:40.832521Z",
     "iopub.status.busy": "2025-05-19T17:46:40.832212Z",
     "iopub.status.idle": "2025-05-19T17:49:52.084879Z",
     "shell.execute_reply": "2025-05-19T17:49:52.076911Z",
     "shell.execute_reply.started": "2025-05-19T17:46:40.832496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (exact sequence match): 0.3447\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model architecture with the best run's hyperparameters\n",
    "best_model = Seq2Seq(\n",
    "    input_size=INPUT_DIM,\n",
    "    output_size=OUTPUT_DIM,\n",
    "    embed_size=best_config['emb_dim'],\n",
    "    hidden_size=best_config['hidden_dim'],\n",
    "    enc_layers=best_config['encoder_layers'],\n",
    "    dec_layers=best_config['decoder_layers'],\n",
    "    dropout=best_config['dropout'],\n",
    "    cell=best_config['cell'].lower()\n",
    ").to(device)\n",
    "\n",
    "# Use best hyperparams & load best model weights\n",
    "model_file_name = f\"model_{best_run.id}.pth\"\n",
    "best_model_file = best_run.file(model_file_name)\n",
    "best_model_file.download(replace=True)\n",
    "\n",
    "# Load the state dict\n",
    "best_model.load_state_dict(\n",
    "    torch.load(model_file_name, map_location=device)\n",
    ")\n",
    "\n",
    "# Print best test accuracy\n",
    "best_model.eval()\n",
    "test_acc = sequence_accuracy(best_model, test_loader, target_idx2char, device)\n",
    "print(f\"Test Accuracy (exact sequence match): {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the prediction and save to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:05:46.616968Z",
     "iopub.status.busy": "2025-05-19T18:05:46.616619Z",
     "iopub.status.idle": "2025-05-19T18:08:55.754940Z",
     "shell.execute_reply": "2025-05-19T18:08:55.747874Z",
     "shell.execute_reply.started": "2025-05-19T18:05:46.616942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions saved to predictions_vanilla.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input_Latin</th>\n",
       "      <th>Target_Bengali</th>\n",
       "      <th>Predicted_Bengali</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7440</th>\n",
       "      <td>licencey</td>\n",
       "      <td>লাইসেন্সে</td>\n",
       "      <td>লাইস্সে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>atmovishvaser</td>\n",
       "      <td>আত্মবিশ্বাসের</td>\n",
       "      <td>আত্মবিহ্বেশের</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>digridhari</td>\n",
       "      <td>ডিগ্রিধারী</td>\n",
       "      <td>দীগৃরীধারী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6232</th>\n",
       "      <td>bhatao</td>\n",
       "      <td>ভাতাও</td>\n",
       "      <td>ভাটাও</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>peronor</td>\n",
       "      <td>পেরোনোর</td>\n",
       "      <td>পেরোনোর</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>petaam</td>\n",
       "      <td>পেতাম</td>\n",
       "      <td>পেতাম</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>nodishomuher</td>\n",
       "      <td>নদীসমূহের</td>\n",
       "      <td>নদীশমূহের</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>uk</td>\n",
       "      <td>উক</td>\n",
       "      <td>উক্ক</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6795</th>\n",
       "      <td>mitratar</td>\n",
       "      <td>মিত্রতার</td>\n",
       "      <td>মিত্রার</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>prajanme</td>\n",
       "      <td>প্রজন্মে</td>\n",
       "      <td>প্রজানমে</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Input_Latin Target_Bengali Predicted_Bengali\n",
       "7440       licencey      লাইসেন্সে           লাইস্সে\n",
       "748   atmovishvaser  আত্মবিশ্বাসের     আত্মবিহ্বেশের\n",
       "3400     digridhari     ডিগ্রিধারী        দীগৃরীধারী\n",
       "6232         bhatao          ভাতাও             ভাটাও\n",
       "4941        peronor        পেরোনোর           পেরোনোর\n",
       "4918         petaam          পেতাম             পেতাম\n",
       "4202   nodishomuher      নদীসমূহের         নদীশমূহের\n",
       "1092             uk             উক              উক্ক\n",
       "6795       mitratar       মিত্রতার           মিত্রার\n",
       "5102       prajanme       প্রজন্মে          প্রজানমে"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decode Sequence\n",
    "def decode_sequence(seq, idx2char):\n",
    "    chars = []\n",
    "    for idx in seq:\n",
    "        if idx2char[idx] == '<eos>':\n",
    "            break\n",
    "        if idx2char[idx] not in ['<pad>', '<sos>']:\n",
    "            chars.append(idx2char[idx])\n",
    "    return ''.join(chars)\n",
    "\n",
    "samples = []\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        src, trg = test_dataset[i]\n",
    "        src = src.unsqueeze(0).to(device)\n",
    "        trg = trg.unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = best_model(src, trg, teacher_forcing_ratio=0)\n",
    "        preds = outputs.argmax(dim=2)\n",
    "\n",
    "        input_seq = decode_sequence(src[0].cpu().numpy(), input_idx2char)\n",
    "        target_seq = decode_sequence(trg[0].cpu().numpy(), target_idx2char)\n",
    "        pred_seq = decode_sequence(preds[0].cpu().numpy(), target_idx2char)\n",
    "\n",
    "        samples.append((input_seq, target_seq, pred_seq))\n",
    "\n",
    "df_samples = pd.DataFrame(samples, columns=['Input_Latin', 'Target_Bengali', 'Predicted_Bengali'])\n",
    "df_samples.to_csv(\"/kaggle/working/predictions_vanilla.csv\", index=False)\n",
    "print(\"Sample predictions saved to predictions_vanilla.csv\")\n",
    "\n",
    "# Show 10 random samples\n",
    "display.display(df_samples.sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!\n",
    "#### Thank You."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 7413986,
     "sourceId": 11805457,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7453953,
     "sourceId": 11862110,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
